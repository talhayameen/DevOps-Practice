###################LINUX#####################

what is echo command ?

ANS: Print argument on screen of terminal

diff b/w relative and absolute path ?

ANS: relative path from current dir , while absolute path is full path,

search a string in a file ?

ANS: grep ,sed , egrep

see permissions of dir and file?

ANS: ll , ls -al , getfacl

difference b/w sudo - and sudo su

ANS: sudo su  doesn't start a subshell with the environment variables of the target user. while sudo su - 
starts subshell 

what is difference between services type k8s"

ClusterIP: This is the default service type in Kubernetes. It exposes the service on an internal IP address, 
NodePort: This service type exposes the service on a static port on each node in the cluster. It creates a mapping between the allocated port on each node and the service. 
LoadBalancer: This service type provisions an external load balancer (such as a cloud provider load balancer) that distributes incoming traffic to the service across multiple nodes. It automatically assigns an external IP address to the service,
ExternalName: This service type is used when you want to provide a DNS alias to an external service. Instead of exposing an internal service, ExternalName maps the service to a DNS name, 


what is redinessprobe and livenessprobe in k8s.
The readinessProbe is used to check if the container is ready to receive traffic. 
The livenessProbe is used to check if the container is still running as expected.


can we set  ReadinessProbe and LivenessProbe

apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      ports:
        - containerPort: 80
      readinessProbe:
        httpGet:
          path: /some/endpoint
          port: 80
        initialDelaySeconds: 15
        periodSeconds: 10
      livenessProbe:
        httpGet:
          path: /healthz
          port: 80
        initialDelaySeconds: 30
        periodSeconds: 15


Docker basic

Docker is a powerful containerization platform that allows you to create, deploy, and manage applications within lightweight, isolated environments called containers. Containers enable you to package an application and its dependencies into a single unit, ensuring consistent behavior across different environments. To understand how Docker works on a Linux machine at a deeper level, let's go through the key components and processes involved:

Docker Engine:

Docker is based on a client-server architecture. The core of Docker is the Docker Engine, which consists of the Docker daemon (dockerd) and the Docker CLI (docker).
The Docker daemon is a long-running process responsible for managing containers, images, networks, and volumes.
The Docker CLI is a command-line tool that interacts with the Docker daemon, allowing users to issue commands to build, run, and manage containers.
Images:

An image is a lightweight, standalone, and executable software package that includes everything needed to run an application (code, runtime, libraries, environment variables, etc.).
Images are built from a set of instructions defined in a Dockerfile, which is a plain text configuration file. Dockerfiles specify the application's base image, installation steps, configurations, and other necessary setup.
Images are stored in a registry (e.g., Docker Hub, private registries), and users can pull images from the registry to their local machine.
Containers:

A container is an instance of an image. It is a runtime environment that runs the application in an isolated manner, using resources from the host machine's kernel.
Containers provide process-level isolation, meaning each container runs in its own isolated process space, file system, and network namespace.
Docker containers are portable and can run on any system that supports Docker, thanks to the containerization technology (Linux namespaces and cgroups) provided by the host OS kernel.
Dockerfile and Build Process:

The Dockerfile is used to define the steps to create an image. The Docker CLI uses this file to build the image using the docker build command.
The build process involves taking a base image, executing the instructions in the Dockerfile, and creating an image layer by layer. Each instruction in the Dockerfile adds a new layer to the image, allowing for better caching and reusability.
Container Lifecycle:

The typical lifecycle of a container involves the following steps:
Create: Use the docker run command to create a new container from an image. This allocates resources and sets up the container's environment.
Start: Use the docker start command to start an existing container.
Stop: Use the docker stop command to stop a running container gracefully.
Pause: Use the docker pause command to pause a running container (freezing its processes).
Unpause: Use the docker unpause command to resume a paused container.
Remove: Use the docker rm command to remove a stopped container.
Docker Networking:

Docker provides a networking model that allows containers to communicate with each other and with the external world.
By default, Docker creates a bridge network that allows containers to connect to it and communicate with each other using container names as hostnames.
Docker also supports host networking and overlay networks for more complex network topologies.
Docker Volumes:

Volumes in Docker are used to persist data generated or used by containers.
Docker volumes provide a way to share data between the host and the container or between containers.
Volumes can be created explicitly using the docker volume command or can be mounted from host directories.
Docker Compose (Optional):

Docker Compose is a tool used to define and run multi-container applications using a YAML file (docker-compose.yml).
The docker-compose.yml file allows you to specify the services, networks, volumes, and other configurations required for your application.
At a deeper level, Docker leverages Linux kernel features such as namespaces, cgroups (control groups), and union file systems to provide container isolation and resource management. Linux namespaces allow Docker to create an isolated environment for each container, including its own process space, network stack, filesystem mounts, etc. Cgroups enable Docker to control and limit the resources (CPU, memory, disk I/O) that containers can use. The union file system (usually overlayfs) is responsible for creating and managing the layered file system of Docker images, providing fast and efficient image building and sharing.

In summary, Docker on a Linux machine works by leveraging Linux kernel features to create isolated containers, each running an application with its own environment and resources. The Docker Engine, Dockerfiles, images, and containers form the core components that enable developers to package and deploy applications consistently across different environments.


What Are Software Deployment Models?

Sure! Here are all the deployment strategies defined in one line each:

Rolling Update: Gradually replaces old instances with new ones, ensuring minimal downtime.
Recreate: Terminates all existing instances and creates new ones simultaneously.
Blue-Green Deployment: Deploys new version alongside the old one, switches traffic when ready.
Canary Deployment: Rolls out new version to a subset of users, monitors, then fully deploys.
Shadow Deployment: Sends traffic to a new version for testing without affecting users.
A/B Testing: Simultaneously runs multiple versions to compare performance.
Feature Toggles: Deploys new features that are activated or deactivated dynamically.
Rolling Back: Reverts to a previous version in case of issues or failures.

Rolling Update: A web application is being updated with a new version. The update is performed by gradually replacing the old containers with the new ones, ensuring that the application remains accessible to users with minimal downtime.

Recreate: A microservice architecture is being updated with a new version of a service. The deployment process involves terminating all instances of the old service and creating new instances with the updated version simultaneously.

Blue-Green Deployment: An e-commerce website is being updated with a new version. The new version is deployed alongside the old version (Blue environment). Once the new version is tested and deemed stable, traffic is switched from the old version to the new one (Green environment).

Canary Deployment: A mobile app is being updated with new features. The new version is first deployed to a small subset of users (e.g., 5% of the user base). The application's performance and user feedback are monitored before rolling out the new version to all users.

Shadow Deployment: A backend service is being updated with a new algorithm for processing data. The new version of the service is deployed alongside the existing one, but all user traffic is still directed to the old version. The new version collects data and performs computations, but the results are discarded (shadow traffic) to compare with the old version's output.

A/B Testing: A social media platform is testing two different layouts for its homepage. The platform simultaneously runs both versions (A and B) for a subset of users. User engagement metrics, such as click-through rates and time spent, are compared to determine which layout performs better.

Feature Toggles: An online shopping application introduces a new payment gateway. The feature is deployed, but it's initially deactivated for all users. The new payment gateway can be dynamically activated for a subset of users to test its functionality and user experience before enabling it for all users.

Rolling Back: An online gaming platform releases an update that introduces a critical bug impacting user experience. The platform quickly reverts to the previous version to maintain service stability while addressing the issue.


###################################################################

basic oop pillars with single line definition

Encapsulation: Encapsulation is the bundling of data and methods that operate on the data into a single unit, known as a class, while restricting access to some of the object's components.

Abstraction: Abstraction is the process of simplifying complex reality by modeling classes based on relevant characteristics and ignoring irrelevant details.

Inheritance: Inheritance is a mechanism where a new class inherits properties and behaviors from an existing class, promoting code reusability and establishing a hierarchical relationship.

Polymorphism: Polymorphism allows objects of different classes to be treated as objects of a common superclass, enabling them to be used interchangeably while responding to method calls specific to their individual types.

These principles form the foundation of Object-Oriented Programming and help in designing modular, organized, and extensible software systems.
